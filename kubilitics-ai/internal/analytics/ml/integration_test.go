package ml

import (
	"math"
	"math/rand"
	"testing"
	"time"
)

// TestMLPipelineIntegration tests the complete ML analytics pipeline
func TestMLPipelineIntegration(t *testing.T) {
	t.Run("End-to-end anomaly detection", func(t *testing.T) {
		// Generate realistic dataset
		data := generateRealisticDataset(1000)

		// Train model
		forest := NewIsolationForest(100, 256, 10)
		err := forest.Fit(data)
		if err != nil {
			t.Fatalf("Failed to train model: %v", err)
		}

		// Detect anomalies
		anomalies := make([]AnomalyResult, 0)
		for _, point := range data {
			result := forest.Predict(point)
			if result.IsAnomaly {
				anomalies = append(anomalies, result)
			}
		}

		// Validate results
		if len(anomalies) == 0 {
			t.Error("Expected to find some anomalies in realistic dataset")
		}

		// Check anomaly severity distribution
		severityCounts := make(map[Severity]int)
		for _, anomaly := range anomalies {
			severityCounts[anomaly.Severity]++
		}

		t.Logf("Found %d anomalies", len(anomalies))
		t.Logf("Severity distribution: %+v", severityCounts)
	})

	t.Run("Concurrent anomaly detection", func(t *testing.T) {
		data := generateRealisticDataset(100)
		forest := NewIsolationForest(50, 64, 8)
		err := forest.Fit(data)
		if err != nil {
			t.Fatalf("Failed to train model: %v", err)
		}

		// Test concurrent predictions
		concurrency := 10
		results := make(chan AnomalyResult, concurrency)
		errors := make(chan error, concurrency)

		for i := 0; i < concurrency; i++ {
			go func(idx int) {
				testPoint := data[idx%len(data)]
				result := forest.Predict(testPoint)
				results <- result
			}(i)
		}

		// Collect results
		successCount := 0
		for i := 0; i < concurrency; i++ {
			select {
			case <-results:
				successCount++
			case err := <-errors:
				t.Errorf("Concurrent prediction failed: %v", err)
			case <-time.After(5 * time.Second):
				t.Fatal("Timeout waiting for concurrent predictions")
			}
		}

		if successCount != concurrency {
			t.Errorf("Expected %d successful predictions, got %d", concurrency, successCount)
		}
	})

	t.Run("Large dataset handling", func(t *testing.T) {
		// Test with large dataset (10,000 points)
		largeData := generateRealisticDataset(10000)

		start := time.Now()
		forest := NewIsolationForest(100, 256, 10)
		err := forest.Fit(largeData)
		if err != nil {
			t.Fatalf("Failed to train on large dataset: %v", err)
		}
		trainingTime := time.Since(start)

		t.Logf("Training time for 10k points: %v", trainingTime)

		// Should complete in reasonable time (< 10 seconds)
		if trainingTime > 10*time.Second {
			t.Errorf("Training took too long: %v", trainingTime)
		}

		// Test prediction speed
		start = time.Now()
		testPoint := largeData[0]
		forest.Predict(testPoint)
		predictionTime := time.Since(start)

		t.Logf("Prediction time: %v", predictionTime)

		// Prediction should be fast (< 100ms)
		if predictionTime > 100*time.Millisecond {
			t.Errorf("Prediction too slow: %v", predictionTime)
		}
	})

	t.Run("Model persistence", func(t *testing.T) {
		// Train model
		data := generateRealisticDataset(500)
		forest1 := NewIsolationForest(100, 256, 10)
		err := forest1.Fit(data)
		if err != nil {
			t.Fatalf("Failed to train model: %v", err)
		}

		// Make prediction with first model
		testPoint := data[0]
		result1 := forest1.Predict(testPoint)

		// Simulate saving and loading (in production would serialize to disk)
		forest2 := forest1 // In real implementation, would serialize/deserialize

		// Make same prediction with "loaded" model
		result2 := forest2.Predict(testPoint)

		// Results should be identical
		if result1.Score != result2.Score {
			t.Errorf("Loaded model produced different score: %f vs %f", result1.Score, result2.Score)
		}
	})
}

// TestARIMAPipelineIntegration tests ARIMA forecasting integration
func TestARIMAPipelineIntegration(t *testing.T) {
	t.Run("End-to-end forecasting", func(t *testing.T) {
		// Generate time series with trend and seasonality
		timeSeries := generateTimeSeriesWithTrend(100)

		// Train ARIMA model
		arima := NewARIMA(2, 1, 2)
		err := arima.Fit(timeSeries)
		if err != nil {
			t.Fatalf("Failed to fit ARIMA model: %v", err)
		}

		// Generate forecast
		forecast, err := arima.Forecast(24)
		if err != nil {
			t.Fatalf("Failed to generate forecast: %v", err)
		}

		// Validate forecast
		if len(forecast.Predictions) != 24 {
			t.Errorf("Expected 24 predictions, got %d", len(forecast.Predictions))
		}

		// Forecast should follow trend
		if len(forecast.Predictions) > 0 {
			lastHistorical := timeSeries[len(timeSeries)-1]
			firstForecast := forecast.Predictions[0]

			// Should be relatively close to last value (within 50%)
			diff := math.Abs(firstForecast - lastHistorical)
			maxDiff := lastHistorical * 0.5
			if diff > maxDiff {
				t.Errorf("Forecast diverges too much from historical: %f vs %f", firstForecast, lastHistorical)
			}
		}

		// Confidence intervals should be reasonable
		if forecast.ConfidenceLevel < 0.8 || forecast.ConfidenceLevel > 0.99 {
			t.Errorf("Unexpected confidence level: %f", forecast.ConfidenceLevel)
		}
	})

	t.Run("Multiple forecasting horizons", func(t *testing.T) {
		timeSeries := generateTimeSeriesWithTrend(100)
		arima := NewARIMA(2, 1, 2)
		err := arima.Fit(timeSeries)
		if err != nil {
			t.Fatalf("Failed to fit ARIMA model: %v", err)
		}

		horizons := []int{6, 12, 24, 48}
		for _, h := range horizons {
			forecast, err := arima.Forecast(h)
			if err != nil {
				t.Errorf("Failed to forecast %d steps: %v", h, err)
				continue
			}

			if len(forecast.Predictions) != h {
				t.Errorf("Expected %d predictions, got %d", h, len(forecast.Predictions))
			}

			// Longer horizons should have wider confidence intervals
			if h == 48 && forecast.ConfidenceLevel > 0.95 {
				t.Log("Warning: Long-term forecast has very high confidence")
			}
		}
	})
}

// TestMLIntegrationWithRealPatterns tests ML with realistic patterns
func TestMLIntegrationWithRealPatterns(t *testing.T) {
	t.Run("CPU spike detection", func(t *testing.T) {
		// Simulate CPU usage with occasional spikes
		data := make([]DataPoint, 0)
		baseTime := time.Now()

		for i := 0; i < 1000; i++ {
			value := 50.0 + math.Sin(float64(i)*0.1)*10 // Normal oscillation

			// Add spikes
			if i%100 == 0 {
				value += 40.0 // Spike
			}

			data = append(data, DataPoint{
				Timestamp: baseTime.Add(time.Duration(i) * time.Minute),
				Value:     value,
				Label:     "cpu_usage",
			})
		}

		forest := NewIsolationForest(100, 256, 10)
		err := forest.Fit(data)
		if err != nil {
			t.Fatalf("Failed to train: %v", err)
		}

		// Should detect spikes
		anomalyCount := 0
		for _, point := range data {
			result := forest.Predict(point)
			if result.IsAnomaly {
				anomalyCount++
			}
		}

		// Should find approximately 10 spikes (we added 10)
		if anomalyCount < 5 || anomalyCount > 20 {
			t.Errorf("Expected ~10 anomalies, found %d", anomalyCount)
		}

		t.Logf("Detected %d CPU spikes out of 10 injected", anomalyCount)
	})

	t.Run("Memory leak pattern", func(t *testing.T) {
		// Simulate gradual memory increase (leak)
		data := make([]DataPoint, 0)
		baseTime := time.Now()

		for i := 0; i < 500; i++ {
			// Gradual increase over time
			value := 1000.0 + float64(i)*2.0 + math.Sin(float64(i)*0.05)*50

			data = append(data, DataPoint{
				Timestamp: baseTime.Add(time.Duration(i) * time.Minute),
				Value:     value,
				Label:     "memory_usage",
			})
		}

		// ARIMA should detect increasing trend
		timeSeries := make([]float64, len(data))
		for i, point := range data {
			timeSeries[i] = point.Value
		}

		arima := NewARIMA(2, 1, 2)
		err := arima.Fit(timeSeries)
		if err != nil {
			t.Fatalf("Failed to fit: %v", err)
		}

		forecast, err := arima.Forecast(24)
		if err != nil {
			t.Fatalf("Failed to forecast: %v", err)
		}

		// Forecast should show continued increase
		lastValue := timeSeries[len(timeSeries)-1]
		forecastMean := 0.0
		for _, pred := range forecast.Predictions {
			forecastMean += pred
		}
		forecastMean /= float64(len(forecast.Predictions))

		if forecastMean <= lastValue {
			t.Error("Expected forecast to show increasing trend for memory leak")
		}

		t.Logf("Memory leak detected: current %.0f, forecast average %.0f", lastValue, forecastMean)
	})
}

// Helper functions

func generateRealisticDataset(size int) []DataPoint {
	data := make([]DataPoint, size)
	baseTime := time.Now()

	for i := 0; i < size; i++ {
		// Normal data: mean=50, stddev=10
		value := 50.0 + (rand.Float64()-0.5)*20

		// Add some outliers (5% of data)
		if rand.Float64() < 0.05 {
			value += (rand.Float64() - 0.5) * 100
		}

		data[i] = DataPoint{
			Timestamp: baseTime.Add(time.Duration(i) * time.Minute),
			Value:     value,
			Label:     "test_metric",
		}
	}

	return data
}

func generateTimeSeriesWithTrend(size int) []float64 {
	timeSeries := make([]float64, size)
	for i := 0; i < size; i++ {
		// Trend + seasonality + noise
		trend := float64(i) * 0.5
		seasonal := math.Sin(float64(i)*0.2) * 10
		noise := (rand.Float64() - 0.5) * 5
		timeSeries[i] = 100 + trend + seasonal + noise
	}
	return timeSeries
}

// BenchmarkMLPipeline benchmarks the complete ML pipeline
func BenchmarkMLPipeline(b *testing.B) {
	data := generateRealisticDataset(1000)

	b.Run("Training", func(b *testing.B) {
		for i := 0; i < b.N; i++ {
			forest := NewIsolationForest(100, 256, 10)
			forest.Fit(data)
		}
	})

	b.Run("Prediction", func(b *testing.B) {
		forest := NewIsolationForest(100, 256, 10)
		forest.Fit(data)
		testPoint := data[0]

		b.ResetTimer()
		for i := 0; i < b.N; i++ {
			forest.Predict(testPoint)
		}
	})

	b.Run("ARIMA Forecasting", func(b *testing.B) {
		timeSeries := generateTimeSeriesWithTrend(100)

		for i := 0; i < b.N; i++ {
			arima := NewARIMA(2, 1, 2)
			arima.Fit(timeSeries)
			arima.Forecast(24)
		}
	})
}
